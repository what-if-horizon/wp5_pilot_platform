# simulation_settings.toml
# Simulation configuration values used by SimulationSession.


### SESSION SETTINGS ### 
# Reproducibility seed 
random_seed = 42
# - session_duration_minutes: max. duration of the simulation session in minutes
session_duration_minutes = 5

### AGENT SPACE ###
# - num_agents: total agents in the session (int)
num_agents = 5 
# - agent_names: corresponding list of agent names to instantiate (list of strings, length must match num_agents). 
agent_names = ["Alice", "Bob", "Charlie", "Lucy", "David"] 

### AGENT PACING ###
# - messages_per_minute: background message rate (int)
messages_per_minute = 5
# - user_response_probability: probability an agent responds when a user message is posted (float between 0 and 1)
user_response_probability = 1 

### AGENT RELATIONSHIPS ###
# Each agent's chance of speaking = chattiness × (1 + attention)
# - chattiness: random 0-1 value assigned at session start (permanent personality trait)
# - attention: dynamic 0-1 value that rises when active and decays over time
#
# attention_decay (0-1): multiplied each tick.
#   → Higher = attention persists longer, active agents stay dominant
#   → Lower = faster cooldown, more rotation between agents
attention_decay = 0.5
# attention_boost_speak (0-1): set attention to this value when agent speaks.
#   → Higher = speakers stay "hot" and likely to speak again
#   → Lower = speaking doesn't strongly favor continued speaking
attention_boost_speak = 0.5
# attention_boost_address (0-1): added to attention when agent is @mentioned.
#   → Higher = mentioned agents more likely to jump in and respond
#   → Lower = mentions have less pull on conversation flow
attention_boost_address = 0.8
# min_weight_floor (0-1): minimum selection weight for any agent.
#   → Higher = more equal participation, quiet agents still speak
#   → Lower = lets natural dominance emerge, some agents may go silent
min_weight_floor = 0.20

### LLM SETTINGS ###
# - llm_provider: which LLM provider to use ("gemini" or "huggingface")
llm_provider = "huggingface"
# - llm_model: model name/identifier for the chosen provider
#   For gemini: e.g., "gemini-2.0-flash"
#   For huggingface: e.g., "meta-llama/Llama-3.1-8B-Instruct"
llm_model = "meta-llama/Llama-3.1-8B-Instruct"
# - context_window_size: how many recent messages to include in prompts (int)
context_window_size = 10
# - llm_concurrency_limit: per-process limit of concurrent LLM requests (must be a positive integer > 0)
llm_concurrency_limit = 5






